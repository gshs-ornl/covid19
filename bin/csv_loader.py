#!/usr/bin/env python

#
# Test program for loading CSV file into the new databse schema
#

import sys
import os
import os.path
import glob
import re
import csv
import psycopg2
import zipfile
import io
import dateparser
import datetime
import json
import collections
import inspect
import fnmatch
import pathlib
import logging
from psycopg2.extras import Json
import argparse

csv.field_size_limit(sys.maxsize)

parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter, description="Loader of CSV scrapes into the new schema")

parser.add_argument("files", type=str, nargs='+', help="CSV files, zip archives, or directories to load; inside zip use name.zip:name.csv")
parser.add_argument("--dsn", help="database to load CSVs to", type=str, default="postgresql://ingester:AngryMoose@localhost:5432/covidb")
parser.add_argument("--schema", help="database schema to load data to", type=str, default='staging')
parser.add_argument("--op", help="operation ('replace' will remove all records for the specific file before appending)", type=str, choices=['append', 'replace', 'new'], default='append')
parser.add_argument("--exclude", help="exclude specified CSVs and zips from processing (globs Ok)", nargs='*', type=str, default=[])
parser.add_argument("--start", help="start processing with the specified CSV or zip (must be an exact match)", type=str)
parser.add_argument("--rows", help="load only specified rows, e.g., -10,11-13,15,17- (use =, intervals inclusive, all files, start row=0, header ignored)", type=str)
parser.add_argument("--datadir", "-C", help="directory with the data", default=".", type=pathlib.Path)
parser.add_argument("--logdir", help="directory for log files", default="logs", type=pathlib.Path)
parser.add_argument("--batch", help="CSV batch name", default=datetime.datetime.now().replace(microsecond=0).isoformat(), type=str)
parser.add_argument("-k", "--dry-run", action="count", default=0, help="dry run, add more k to do more")
parser.add_argument("-v", "--verbosity", action="count", default=0, help="add more v to increase verbosity, e.g., -vvvv")

args = parser.parse_args()

print(f"Loading from {args.files} to {args.dsn} schema={args.schema}")

logdir = pathlib.Path(args.logdir, args.batch)
logdir.mkdir(parents=True, exist_ok=True)

# logging for bulk errors
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
con_h = logging.StreamHandler(sys.stdout)
if args.verbosity == 1:
    con_h.setLevel(logging.INFO)
elif args.verbosity == 2:
    con_h.setLevel(logging.WARNING)
elif args.verbosity > 2:
    con_h.setLevel(logging.ERROR)
logger.addHandler(con_h)

def lno():
    """current line number (for debugging)
    :returns: file name and line number

    """
    return sys.argv[0] + ':' + str(inspect.currentframe().f_back.f_lineno)

def create_attribute(dataset, attr, name=f"FIXME: name autogenerated by {sys.argv[0]}", meta=None, ignore_duplicate=False):
    """Inserts new attribute name ignoring KeyViolation.  Will add % to meta if 'percent' is in the name.

    :attr: attribute name
    :meta: attribute metadata
    """

    # add '%' to metadata if name contains 'percent'
    if 'percent' in attr_name:
        if meta is None:
            meta = {}
        meta['type'] = '%'

    # FIXME: do an explicit check on attribute presence possibly in memory
    # FIXME: check for if metadata is compatible
    try:
       cur.execute(f"INSERT INTO {args.schema}.attributes(dataset_id, attr, name, meta) VALUES (%s, %s, %s, %s)", (dataset, attr, name, Json(meta) if meta else None))
    except psycopg2.IntegrityError as e:
        if ignore_duplicate:
            conn.rollback()
        else:
            raise e
    else:
        conn.commit()

simple_attrs = "cases|deaths|presumptive|recovered|tested|hospitalized|negative|severe|monitored|no_longer_monitored|pending|active|inconclusive|quarantined".split("|")

def is_good_csv_name(s):
    """Check if the file name look like a good CSV, avoid system and hidden files.

    :s: file name to check
    :returns: True or False

    """

    return s.lower().endswith(".csv") \
        and not "__MACOSX/" in s \
        and not "/." in s \
        and not s.startswith(".")

def next_csv():
    """returns next available CSV

    will use files.args
    if directory -- globs all CSV file
    if CSV file -- process
    if zip file -- process all CSV files in it
    :returns: stream with a CSV, file name

    """

    # creating a lisy of included files
    excl = {}
    for ff in args.exclude:
        ff_parts = re.split("(\.zip):", ff, maxsplit=3, flags=re.IGNORECASE)
        ff_zip = ff_parts[0] + (ff_parts[1] if len(ff_parts) > 1 else '')
        if not ff_zip in excl:
            excl[ff_zip] = []
        if len(ff_parts) > 2:
            excl[ff_zip].append(ff_parts[2])

    # TODO: support globs in args.files
    for ff in [ pathlib.PurePath(args.datadir, p) for p in args.files ]:

        if any([fnmatch.fnmatch(ff, ef) for ef in args.exclude]): # GLOBS!!
            print(f"Skipping excluded file {ff}...")
            continue

        if os.path.isdir(ff):
            print(f"Looking for CSV files in dir {ff} ...")
            for fn in glob.glob(str(ff) + "/*.csv") + glob.glob(str(ff) + "/*.CSV") :
                if any([fnmatch.fnmatch(fn, ef) for ef in args.excludei]):
                    print(f"Skipping excluded file {ff}...")
                else:
                    yield open(fn, "r", encoding='utf-8-sig', errors='replace'), fn

        elif ff.suffix.lower() == ".csv":
            yield open(ff, "r", encoding='utf-8-sig', errors='replace'), ff

        # TODO: option to select a file inside a zip
        elif ff.suffix.lower() == ".zip" or re.search(".zip:", ff, flags=re.IGNORECASE):

            # extrascting zip and csv fnames for synatx like x.zip:y.csv
            ff_parts = re.split("(\.zip):", str(ff), flags=re.IGNORECASE)
            ff_zip = ff_parts[0] + ( ff_parts[1] if len(ff_parts) > 1 else '' )
            ff_csv = ff_parts[2] if len(ff_parts) > 2 else None

            # excludes
            excl_ent = next((fe for fe in excl.keys() if fnmatch.fnmatch(ff_zip, fe)), None)
            if excl_ent and not excl[excl_ent]: # empty list of files in the archive
                print(f"Skipping {ff_zip} (glob {excl_ent})")
                continue

            with zipfile.ZipFile(ff_zip, "r") as zf:
                for fn in [fn for fn in zf.namelist() if is_good_csv_name(fn)]:
                    if not ff_csv or fnmatch.fnmatch(fn, ff_csv):
                        if excl_ent and any([fnmatch.fnmatch(fn, fe) for fe in excl[excl_ent]]):
                            print(f"Skipping {ff_zip}:{fn} (glob {excl_ent}:{excl[excl_ent]})")
                        elif any([fnmatch.fnmatch(fn, fe) for fe in args.exclude]):
                            print(f"Skipping {ff_zip}:{fn} (glob in {args.exclude}")
                        else:
                            with zf.open(fn) as csvh:
                                yield io.TextIOWrapper(csvh, encoding='utf-8-sig', errors='replace'), f"{ff}:{fn}"
        else:
            print(f"Do not know what to do with {ff}")

def store_value_tuple(scrape_id, geounit_id, vtime, attr, val, csv_row=None, csv_col=None):
    """Store value tuple in the database

    :scrape_id: identifier for the scrape
    :geounit_id: identifier for the region
    :valid_time: time of observation
    :attr: name of the attributes
    :val: value of the attribute (text)
    :meta: attribute metadata (percent, UoM, etc.)
    :csv_row, csv_col: back reference to original CSV for debugging, will be stored in the parser column

    """

    try:

        # FIXME: in the code I presume that all metadata is inherited from the attributes table
        cur.execute(f"""
            INSERT INTO {args.schema}.stav(scrape_id, geounit_id, vtime, attr, val, csv_row, csv_col)
            VALUES (%s, %s, %s, %s, %s, %s, %s)
            """,
            ( scrape_id, geounit_id, vtime, attr, val, csv_row, csv_col ))

    except psycopg2.IntegrityError as e:
        # this is needed to collect errors while processing all files
        logger.error(f"IntegrityError in file {fname}:{row_no} from {sys.argv[0]}:{inspect.currentframe().f_back.f_lineno}, failure on value={val}: {e}")
        conn.rollback()

    else:
        conn.commit()

def mk_attr_name(row, *arg_parts, check_prc=False):
    """assembles attribute clean name from multiple part, creates attrribute extension

    :row: OrderedDist with the CSV row
    :*parts: parts of the name being created, names prepended with = will be treated as literals,
     all others will be used as index to row
    :check_prc: if true 'percent' column will be checked and the name and ext modified accrodibgly
    :attr_ext: attribute extension will be created and/or appended from attribute name
    :returns: attr_name

    """

    parts = list(arg_parts)

    for i, part in enumerate(parts):
        if part.startswith('='):
            parts[i] = part[1:].strip().lower()
        else:
            # remove excessive spaces in the attribute names
            parts[i] = " ".join(row[part].strip().lower().split())

    if check_prc and row['percent']:
        if row['percent'].strip().lower() == 'yes':
            parts.append('percent')
        else:
            logger.warning(f"The value of 'percent' column is not Yes or empty {row['percent']}")

    attr_name='_'.join(parts)

    return attr_name

def same_ign_none(l1, l2):
    """Compares 2 lists element by element ignoring positions with None values

    :l1: 1st list
    :l2: 2nd list
    :returns: True if allelements are the same, nones ignored

    """

    assert len(l1) == len(l2)

    return all([l1[i] == l2[i] for i,_ in enumerate(l1) if not l1[i] is None and not l2[i] is None])

# parser for row selector
rows = []
if args.rows:
    for r in (r.split('-') for r in args.rows.split(",")):
        if len(r) == 1:
            rows.append((int(r[0]),))
        else:
            rows.append(( int(r[0]) if r[0] else 0, int(r[1]) if r[1] else sys.maxsize ))

    rows.sort(key=lambda _ : _[0])

with psycopg2.connect(args.dsn) as conn:
    with conn.cursor() as cur:

        logfh = None

        for csv_stream, fname in next_csv():

            if args.start and args.start == fname:
                args.start = None

            if args.start:
                print(f"Skipping before start {fname}")
                continue

            # open CSV-file specific log file
            if not logfh is None:
                logger.removeHandler(logfh)

            fname_log = pathlib.Path(\
                 logdir,\
                 str(pathlib.Path(fname).relative_to(args.datadir)).replace('/', '%').replace('..','')\
            ).with_suffix(".log")
            print(f"Logging into {fname_log}...")
            logfh = logging.FileHandler(fname_log)
            logfh.setLevel(logging.DEBUG)
            logger.addHandler(logfh)
            logger.info("Parsing {} started at {}".format(fname, datetime.datetime.now().isoformat()) )
            try:

                print(f"\nOpening {fname}")
                csvr = csv.DictReader(csv_stream)

                # reset file-level variables
                simpl_vals_prev = []
                id_vals_prev = [] # set of values that identifies a scrape
                row_prev = None # a copy of the previous row
                group_row = 0
                group_type = group_type_prev = group_hospital_name = None
                provider = vendor = dataset = None
                current_range = 0

                if args.dry_run:
                    print("...dry run...")
                    continue

                if args.op == 'replace':
                    # FIXME: transaction isolation --- the count of deleted data points is not correct when multiple processes are running
                    cur.execute(f"SELECT count(*) from {args.schema}.stav")
                    cnt = cur.fetchone()[0]
                    cur.execute(f"""
                        DELETE FROM {args.schema}.scrapes
                        WHERE csv_file = %s
                        """,
                        (fname,))
                    cur.execute(f"SELECT count(*) from {args.schema}.stav")
                    cnt -= cur.fetchone()[0]
                    logger.info(f'Removed {cnt} data points previously loaded from this file')
                    conn.commit()
                elif args.op == 'new':
                    # load only CSVs not already loaded
                    cur.execute(f"""
                        SELECT *
                        FROM {args.schema}.scrapes
                        WHERE csv_file = %s
                        """,
                        (fname,))
                    if cur.rowcount:
                        logger.info(f"{fname} already loaded, skipping...")
                        continue

                for row_no, raw_row in enumerate(csvr):
                    # row_no is not the same as csvr.line_num

                    # cleanup empty strings and lower case keys
                    row = collections.OrderedDict()
                    for k,v in raw_row.items():
                        if len(v.strip()) > 0:
                            row[k.lower()] = v.strip()
                        else:
                            row[k.lower()] = None

                    # actions before the 1st row
                    if row_no == 0:
                        # check for essential columns once per file
                        try:
                            for c in ('access_time', 'state' ):
                                _ = row[c]
                        except KeyError as e:
                            logger.warn(f"Column '{c}' not found in {fname}, file skipped")
                            break

                    # skip row if requestd
                    if rows:
                        crange = rows[current_range]
                        if not ( len(crange)==1 and row_no == crange[0] or row_no >= crange[0] and row_no <= crange[1] ):
                            logger.info(f"\rSkipped CSV row {row_no}  ", end='', flush=True)
                            continue
                        if len(crange) == 1 or row_no == crange[1]:
                            current_range += 1
                            if current_range >= len(rows):
                                logger.info("The rest of the file skipped")
                                break

                    # skip all rows with all empty values
                    if not any(row.values()):
                        logger.warn(f"Skipped empty row in {fname}:{row_no}")
                        continue

                    # fill columns missing in manual scrapes
                    # TODO: factor this out into a separate function
                    if not row['access_time']:
                        logger.error(f"Value in 'access_time' is None, skipping row {fname}:{row_no}")
                        continue
                    if not 'provider' in row or row['provider'] == 'state':
                        row['provider'] = 'doe-covid19'
                    if not 'country' in row:
                        row['country'] = 'US'
                    if 'other value' in row:
                        row['other_value'] = row['other value']
                        del row['other value']
                    if 'quarantine' in row:
                        row['quarantined'] = row['quarantine']
                        del row['quarantine']
                    # these are columns missing either in manually or automatically scraped CSVs
                    for c in ('resolution', 'no_longer_monitored', 'page', 'pending', 'quarantined', 'percent', 'county'):
                        if not c in row:
                            row[c] = None

                    # save provider, dataset, and vendor
                    if row['provider'] != provider:
                        provider = row['provider']
                        try:
                            cur.execute(f"""
                                INSERT INTO {args.schema}.providers (provider_id)
                                VALUES (%s)
                                """,
                                (provider, ))
                        except psycopg2.IntegrityError as e:
                            conn.rollback()
                        else:
                            conn.commit()

                        if provider == 'doe-covid19':
                            vendor = 'US^' + row['state']
                            dataset = vendor + ':COVID19'
                        else:
                            vendor = dataset = provider

                        try:
                            cur.execute(f"""
                                INSERT INTO {args.schema}.vendors (vendor_id, name)
                                VALUES (%s, 'FIXME: name autogenerated by {sys.argv[0]}')
                            """,
                            (vendor,))
                        except psycopg2.IntegrityError as e:
                            conn.rollback()
                        else:
                            conn.commit()

                        try:
                            cur.execute(f"""
                                INSERT INTO {args.schema}.datasets (vendor_id, dataset_id, name)
                                VALUES (%s, %s, 'FIXME: name autogenerated by {sys.argv[0]}')
                            """,
                            (vendor, dataset,))
                        except psycopg2.IntegrityError as e:
                            conn.rollback()
                        else:
                            conn.commit()

                    # set times
                    try:
                        parsed_time = dateparser.parse(row['access_time'])
                        if parsed_time is None:
                            raise ValueError()
                        else:
                            row['access_time'] = parsed_time
                    except ValueError as e:
                        logger.error(f"Unparseable time in 'access_time' {fname}:{row_no}, row skipped:", row['access_time'], e)
                        continue
                    # extract day of the record, the logic
                    #   * use 'updated' if avavilable
                    #     * can be in jason format
                    #     * if missing in some records use one found for the file
                    #   * otherwise use access_time
                    #  TODO: * if not avavilable extract from the file name
                    valid_time = None
                    if row['updated'] is None and id_vals_prev:
                        # get valid time from the previos row
                        valid_time = id_vals_prev[2]
                    elif row['updated']:
                        if row['updated'].strip().startswith('{'):
                            try:
                                jd = json.loads(row['updated'].replace("'", '"'))
                                valid_time = datetime.date(jd['year'], jd['month'], jd['day'])
                            except json.decoder.JSONDecodeError as e:
                                logger.error(f"Unparseable 'updated' JSON in {fname}:{row_no}, row skipped:", row['updated'], e)
                                continue
                        else:
                            try:
                                ts_for_valid_time = dateparser.parse(row['updated'])
                                if ts_for_valid_time is None:
                                    raise ValueError()
                                else:
                                    valid_time = ts_for_valid_time.date()
                            except ValueError as e:
                                logger.error(f"Unparseable time in 'updated' {fname}:{row_no}, row skipped:", row['updated'], e)
                                continue
                    # no good valid_time
                    if valid_time is None:
                        valid_time = row['access_time'].date()

                    # insert scrapes
                    # I am doing explicit check on the row to prevent filling postgresql logs
                    if not row['url']:
                        logger.error(f"Missing URL in {fname}:{row_no}, row skipped: {row}")
                        continue
                    cur.execute(f"""
                        SELECT scrape_id
                        FROM {args.schema}.scrapes
                        WHERE provider_id = %s AND uri = %s AND scraped_ts = %s
                        """,
                        (row['provider'], row['url'], row['access_time']))
                    if not cur.rowcount: # this means "not found"
                        cur.execute(f"""
                            INSERT INTO {args.schema}.scrapes(provider_id, uri, dataset_id, scraped_ts, doc, csv_file, csv_row)
                            VALUES (%s, %s, %s, %s, %s, %s, %s)
                            RETURNING scrape_id
                            """,
                            (row['provider'], row['url'], dataset, row['access_time'], row['page'], fname, row_no) )
                        conn.commit()
                    scrape_id = cur.fetchone()[0]

                    # detect row group type based on values in columns
                    if group_type == 'other.hospital': # this is another kind of groupping
                        # FIXME: assumption here is that hospital groups go until the end of the file
                        if row['other'] == 'HospitalName':
                            group_hospital_name = row['other_value']
                            group_row=0
                            continue
                    elif row['age_range']:
                        #   1.  age group
                        #   1.1 includes age-sex subgroup
                        group_type='age'
                        if row['sex']:
                            group_type='age.sex'
                        elif row['other']:
                            group_type='age.other'
                    elif row['sex']:
                        #   2.  sex group (does not include rows in age group)
                        group_type='sex'
                        if row['other']:
                            group_type='sex.other'
                    elif row['other'] and row['other'] == 'HospitalName':
                        group_type='other.hospital'
                        group_hospital_name = row['other_value']
                        group_row = 0
                        continue
                    else:
                        #   0.  no group
                        group_type=None
                    # future groups
                    #   3.  other group
                    #   4.  ? hospital group

                    # 'region' is assmebled from country-state-county
                    geounit_id = "^".join([row[c].upper() for c in ('country', 'state', 'county') if row[c]])
                    if row['region']:
                        geounit_id += "$" + row[ 'region' ]
                    if group_type == 'other.hospital':
                        geounit_id += "$" + group_hospital_name
                        row['resolution'] = 'hospital'
                    try:
                        cur.execute(f"""
                            INSERT INTO {args.schema}.geounits (geounit_id, resolution)
                            VALUES (%s, %s)
                            """,
                            (geounit_id, row['resolution']) )
                    except psycopg2.IntegrityError as e:
                        conn.rollback()
                    else:
                        conn.commit()

                    # make a copy of the whole row
                    id_vals = [scrape_id, geounit_id, valid_time]
                    simpl_vals = id_vals + [row[r] for r in simple_attrs]
                    if row_no == 0: # 1st row in the CSV file
                        id_vals_prev = id_vals
                        simpl_vals_prev = simpl_vals
                        row_prev = row

                    # reset group row count if
                    #  * group type has changed
                    #  * new file
                    #  * new scrape
                    #  * new region
                    #  * new time
                    #  * no group
                    if id_vals != id_vals_prev \
                            or group_type is None:
                            #or group_type != group_type_prev:
                        group_row = 0
                    else:
                        group_row += 1

                    # special case: age groups
                    if group_type and group_type.startswith('age'):

                        # collect values from age_* columns
                        for attr in [a for a in row.keys() if row[a]]:
                            if attr.startswith("age_") and attr != 'age_range':

                                attr_name = mk_attr_name( row, "=age", 'age_range', '='+attr[4:] )
                                create_attribute(dataset, attr_name, ignore_duplicate=True)
                                store_value_tuple(scrape_id, geounit_id, valid_time, attr_name, row[attr], row_no, attr)

                        if group_type == 'age':
                            for attr in [a for a in simple_attrs if row[a]]:
                                if group_row==0:
                                # load simple_attrs value from the 1st row for all subtypes of group 'age'
                                    attr_name = mk_attr_name( row, '='+attr, check_prc=True )
                                    create_attribute(dataset, attr_name, ignore_duplicate=True)
                                    store_value_tuple(scrape_id, geounit_id, valid_time, attr, row[attr], row_no, attr)
                                elif row[attr] != row_prev[attr]:
                                    logger.warn(f"None-repeating attribute '{attr}' ({row[attr]} vs {row_prev[attr]}) in group '{group_type}' in {fname}:{row_no} " + lno())

                        elif group_type == 'age.sex':
                            # process age-specific sex here
                            for attr in [ a for a in row.keys() if row[a] ]:
                                if attr.startswith("sex_"):

                                    attr_name = mk_attr_name( row, "=age", "age_range", 'sex', '='+attr[4:] )
                                    create_attribute(dataset, attr_name, ignore_duplicate=True)
                                    store_value_tuple(scrape_id, geounit_id, valid_time, attr_name, row[attr], row_no, attr)

                                elif attr in simple_attrs:
                                    # this is the case when both age_range and sex then simple_attrs are applied to age_range
                                    if group_row==0:

                                        attr_name = mk_attr_name( row, "=age", 'age_range', '='+attr, check_prc=True)
                                        create_attribute(dataset, attr_name, ignore_duplicate=True)
                                        store_value_tuple(scrape_id, geounit_id, valid_time, attr, row[attr], row_no, attr)

                                    elif row[attr] != row_prev[attr]:
                                        # skip repeating values in other than the 1st row
                                        logger.warn(f"None-repeating attribute '{attr}' ({row[attr]} vs {row_prev[attr]}) in group '{group_type}' in {fname}:{row_no} " + lno())
                                        logger.warn(f"prev: {simpl_vals_prev}")
                                        logger.warn(f"curr: {simpl_vals}")

                        elif group_type == 'age.other':
                            # process 'other' groupings like comorbidities
                            if row['other_value']:
                                try:
                                    float(row['other_value'])
                                    # treat values in other_value and simple_attrs as a normal row
                                    if group_row == 0:
                                        for attr in [ a for a in simple_attrs if row[a] ]:

                                            attr_name = mk_attr_name( row, '='+attr, check_prc=True)
                                            create_attribute(dataset, attr_name, ignore_duplicate=True)
                                            store_value_tuple(scrape_id, geounit_id, valid_time, attr_name, row[attr], row_no, attr)

                                        attr_name = mk_attr_name( row, '=age', 'age_range', 'other')
                                        create_attribute(dataset, attr_name, ignore_duplicate=True)
                                        store_value_tuple(scrape_id, geounit_id, valid_time, attr_name, row['other_value'], row_no, 'other_value')

                                    elif not same_ign_none(simpl_vals, simpl_vals_prev):
                                        logger.warn(f"None-repeating attribute in group '{group_type}' in {fname}:{row_no} " + lno())
                                        logger.warn(f"prev: {simpl_vals_prev}")
                                        logger.warn(f"curr: {simpl_vals}")

                                except ValueError as e:
                                    # treat other_value as a mofifier simple attributes
                                    for attr in [ a for a in simple_attrs if row[a] ]:

                                        attr_name = mk_attr_name( row, '=age', 'age_range', 'other', 'other_value', '='+attr, check_prc=True)
                                        create_attribute(dataset, attr_name, ignore_duplicate=True)
                                        store_value_tuple(scrape_id, geounit_id, valid_time, attr_name, row[attr], row_no, attr)
                            else:
                                # other_value is empty, treta the content of 'other' as a modifier for simple_attrs
                                for attr in [ a for a in simple_attrs if row[a] ]:

                                    attr_name = mk_attr_name( row, '=age', 'age_range', 'other', '='+attr, check_prc=True)
                                    create_attribute(dataset, attr_name, ignore_duplicate=True)
                                    store_value_tuple(scrape_id, geounit_id, valid_time, attr_name, row[attr], row_no, attr)

                        else:
                            logger.critical(f"BUG: Unknown age subgroup {group_type}")
                            raise ValueError(f"BUG: Unknown age subgroup {group_type}")

                    # special case: sex
                    elif group_type and group_type.startswith('sex'):

                        for attr in [ a for a in row.keys() if row[a] ]:
                            if attr.startswith("sex_"):

                                attr_name = mk_attr_name( row, "=sex", 'sex', '='+attr[4:] )
                                create_attribute(dataset, attr_name, ignore_duplicate=True)
                                store_value_tuple(scrape_id, geounit_id, valid_time, attr_name, row[attr], row_no, attr)

                        if group_type == 'sex':
                            for attr in [a for a in simple_attrs if row[a]]:
                                if group_row==0:
                                # load simple_attrs value from the 1st row for all subtypes of group 'age'
                                    attr_name = mk_attr_name( row, '='+attr, check_prc=True )
                                    create_attribute(dataset, attr_name, ignore_duplicate=True)
                                    store_value_tuple(scrape_id, geounit_id, valid_time, attr, row[attr], row_no, attr)
                                elif row[attr] != row_prev[attr]:
                                    logger.warn(f"None-repeating attribute '{attr}' in group '{group_type}' in {fname}:{row_no} " + lno())

                        elif group_type == 'sex.other':

                            if row['other_value']:
                                try:
                                    float(row['other_value'])
                                    # treat other_value as a value for attribute defined in column 'other'
                                    if group_row == 0:
                                        for attr in [ a for a in simple_attrs if row[a] ]:

                                            attr_name = mk_attr_name( row, '='+attr, check_prc=True)
                                            create_attribute(dataset, attr_name, ignore_duplicate=True)
                                            store_value_tuple(scrape_id, geounit_id, valid_time, attr_name, row[attr], row_no, attr)

                                        attr_name = mk_attr_name( row, '=sex', 'sex', 'other')
                                        create_attribute(dataset, attr_name, ignore_duplicate=True)
                                        store_value_tuple(scrape_id, geounit_id, valid_time, attr_name, row['other_value'], row_no, 'other_value')

                                    elif not same_ign_none(simpl_vals, simpl_vals_prev):
                                        logger.warn(f"None-repeating attribute in group '{group_type}' in {fname}:{row_no} " + lno())
                                        logger.warn(f"prev: {simpl_vals_prev}")
                                        logger.warn(f"curr: {simpl_vals}")

                                except Exception as e:

                                    # look for corresponding values in other columns
                                    attr_name = mk_attr_name( row, "=sex", "sex", 'other', 'other_value', check_prc=True )
                                    create_attribute(dataset, attr_name, ignore_duplicate=True)
                                    store_value_tuple(scrape_id, geounit_id, valid_time, attr_name, row[attr], row_no, attr)

                            else: # empty other_value

                                for attr in [ a for a in simple_attrs if row[a] ]:

                                    attr_name = mk_attr_name(row, '=sex', 'sex', 'other', '='+attr, check_prc=True )
                                    create_attribute(dataset, attr_name, ignore_duplicate=True)
                                    store_value_tuple(scrape_id, geounit_id, valid_time, attr_name, row[attr], row_no, attr)
                        else:
                            logger.critical(f"BUG: Unknown age subgroup {group_type}")
                            raise ValueError(f"BUG: Unknown age subgroup {group_type}")

                    elif group_type == 'other.hospital':

                        if row['other_value']:
                            attr_name = mk_attr_name( row, 'other' )
                            create_attribute(dataset, attr_name, ignore_duplicate=True)
                            store_value_tuple(scrape_id, geounit_id, valid_time, attr_name, row['other_value'], row_no, 'other_value')

                        if group_row > 0 and not same_ign_none(simpl_vals, simpl_vals_prev):
                            logger.warn(f"None-empty or none-repeating simple attribute values in '{group_type}' group in {fname}:{row_no} " + lno())
                            print(f"prev: {simpl_vals_prev}")
                            print(f"curr: {simpl_vals}")

                    elif group_type is None:
                        # special case: other outside of age and sex
                        if row['other'] and row['other_value']:
                            try:
                                float(row['other_value'])
                                # treat 'other_value' as value

                                attr_name = mk_attr_name(row, 'other', check_prc=True )
                                create_attribute(dataset, attr_name, ignore_duplicate=True)
                                store_value_tuple(scrape_id, geounit_id, valid_time, attr_name, row['other_value'], row_no, 'other')

                            except ValueError as e:
                                # treat 'other' as modifier for other columns
                                for attr in simple_attrs:
                                    if row[attr]:

                                        attr_name = mk_attr_name( row, 'other', 'other_value', '='+attr, check_prc=True)
                                        create_attribute(dataset, attr_name, ignore_duplicate=True)
                                        store_value_tuple(scrape_id, geounit_id, valid_time, attr_name, row[attr], row_no, attr)

                        elif row['other']:
                            pass
                            # print(f"None-empty 'other' while 'other_value' is empty in {fname}:{row_no}")
                        else:
                            # the bulk of the values comes from here
                            for attr in simple_attrs:
                                if row[attr]:

                                    attr_name = mk_attr_name( row, '='+attr, check_prc=True)
                                    create_attribute(dataset, attr_name, ignore_duplicate=True)
                                    store_value_tuple(scrape_id, geounit_id, valid_time, attr_name, row[attr], row_no, attr)

                    simpl_vals_prev = simpl_vals
                    id_vals_prev = id_vals
                    group_type_prev = group_type
                    row_prev = row

                    if args.verbosity > 0:
                        print(f"\rCSV row {row_no}  ", end='', flush=True)
                if args.verbosity > 0:
                    print()
                print(f"Finished {fname}")
                logger.info("Parsing of {} completed at {}".format(fname, datetime.datetime.now().isoformat()) )
            except IOError as e:
                logger.error(f"Failure in {fname}: {e}")
            except zipfile.BadZipfile as e:
                logger.error(f"Bad zip, skipping the rest of {fname} after line {row_no}: {e}")

    conn.commit()

conn.close()

